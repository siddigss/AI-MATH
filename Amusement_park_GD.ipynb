{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "  Now that we understand the steps for gradient descent, let us do an example:\n",
        "  A new amusement park is about to open in Jeddah. The engineers would like to build a fun wiggly ride that shows you all the cool places in the park. The ride they came up with had the following route:\n",
        "\n",
        "  $$g(x,t)=t \\sin{x}+3t \\cos (2x).$$\n",
        "  They also know that the cool places are located at the points $$(x,y)=\\{(1,-3),(3,6),(5,-7),(7,2),(9,5),(11,-8)\\}.$$\n",
        "  Unfortunately, they do not know what is the best $t$ that would pass along most of the cool places. Can you help them?\n",
        "  play with this graph to better understand the problem:\n",
        "  https://www.desmos.com/calculator/jyp7hitvh2"
      ],
      "metadata": {
        "id": "0Sr42m5Itj1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math"
      ],
      "metadata": {
        "id": "sgMvBcSZtNRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We wanted to find the best $t$ for the engineers. Can you think of a loss function that depends only on $t$ that we would like to minimize?"
      ],
      "metadata": {
        "id": "DgqpvF4fsYlb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In these cases, we usually use the *squared loss function*, which comes in the following form:\n",
        "$$loss(t)=(g(x_1,t)-y_1)^2+(g(x_2,t)-y_2)^2+(g(x_3,t)-y_3)^2+(g(x_4,t)-y_4)^2+(g(x_5,t)-y_5)^2+(g(x_6,t)-y_6)^2$$\n",
        "where $(x_1,y_1)$ stands for the first location $(1,-3)$ and $(x_2,y_2)=(3,6)$ and so on and so forth."
      ],
      "metadata": {
        "id": "BWrA1mTqthCc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To minimize this function we need to calculate its derivative. Can you do that?"
      ],
      "metadata": {
        "id": "schk-wADv81p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "points=[(1,-3),(3,6),(5,-7),(7,2),(9,5),(11,-8)]\n",
        "def Loss(t,points):\n",
        "\n",
        "  loss=0\n",
        "\n",
        "  for point in points:\n",
        "    loss+=(t*math.sin(point[0])+3*t*math.cos(2*point[0])-point[1])**2\n",
        "\n",
        "  return loss\n",
        "\n",
        "def Loss_derivative(t,points):\n",
        "\n",
        "  loss_derivative=0\n",
        "  for point in points:\n",
        "    loss_derivative+=2*(t*math.sin(point[0])+3*t*math.cos(2*point[0])-point[1])*(math.sin(point[0])+3*math.cos(2*point[0]))\n",
        "  return loss_derivative"
      ],
      "metadata": {
        "id": "k4desbhRwGYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code above defines the loss function and the derivative of the loss function. What are the next steps of gradient descent?\n"
      ],
      "metadata": {
        "id": "WvW4u6HXy8JF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate=0.001\n",
        "t=7\n",
        "for i in range(150):\n",
        "  t=t-learning_rate*Loss_derivative(t,points)\n",
        "  print('t=',t,'Loss_derivative=',Loss_derivative(t,points))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJeGCgYxx883",
        "outputId": "f34bbb8e-3cf9-474f-e32e-b6b9af43c558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "t= 6.560147518338105 Loss_derivative= 400.9307363815552\n",
            "t= 6.159216781956549 Loss_derivative= 365.4531054777983\n",
            "t= 5.793763676478751 Loss_derivative= 333.11482554000315\n",
            "t= 5.460648850938748 Loss_derivative= 303.63810111688343\n",
            "t= 5.157010749821865 Loss_derivative= 276.7697183708656\n",
            "t= 4.880241031450999 Loss_derivative= 252.27886989584692\n",
            "t= 4.627962161555152 Loss_derivative= 229.9551720128688\n",
            "t= 4.398006989542282 Loss_derivative= 209.60685751168648\n",
            "t= 4.188400132030596 Loss_derivative= 191.0591283133469\n",
            "t= 3.997341003717249 Loss_derivative= 174.1526539026554\n",
            "t= 3.8231883498145938 Loss_derivative= 158.74220263162044\n",
            "t= 3.6644461471829732 Loss_derivative= 144.69539413636355\n",
            "t= 3.5197507530466097 Loss_derivative= 131.8915621503864\n",
            "t= 3.3878591908962234 Loss_derivative= 120.22071794542066\n",
            "t= 3.267638472950803 Loss_derivative= 109.58260549551044\n",
            "t= 3.1580558674552925 Loss_derivative= 99.8858402479045\n",
            "t= 3.058170027207388 Loss_derivative= 91.04712410254434\n",
            "t= 2.9671229031048436 Loss_derivative= 82.99052985658817\n",
            "t= 2.8841323732482556 Loss_derivative= 75.6468489671359\n",
            "t= 2.8084855242811195 Loss_derivative= 68.95299702924355\n",
            "t= 2.739532527251876 Loss_derivative= 62.85147186210523\n",
            "t= 2.676681055389771 Loss_derivative= 57.2898595482028\n",
            "t= 2.619391195841568 Loss_derivative= 52.220384182151236\n",
            "t= 2.567170811659417 Loss_derivative= 47.59949746144939\n",
            "t= 2.5195713141979676 Loss_derivative= 43.38750459359774\n",
            "t= 2.47618380960437 Loss_derivative= 39.54822330601442\n",
            "t= 2.4366355862983555 Loss_derivative= 36.04867302954257\n",
            "t= 2.400586913268813 Loss_derivative= 32.858791585543685\n",
            "t= 2.3677281216832693 Loss_derivative= 29.951176942833982\n",
            "t= 2.3377769447404355 Loss_derivative= 27.300851826079363\n",
            "t= 2.310476092914356 Loss_derivative= 24.885049153564747\n",
            "t= 2.2855910437607916 Loss_derivative= 22.68301646118516\n",
            "t= 2.2629080272996065 Loss_derivative= 20.675837632600874\n",
            "t= 2.2422321896670057 Loss_derivative= 18.846270404167342\n",
            "t= 2.2233859192628382 Loss_derivative= 17.178598248757563\n",
            "t= 2.2062073210140807 Loss_derivative= 15.658495366115684\n",
            "t= 2.190548825647965 Loss_derivative= 14.27290361996785\n",
            "t= 2.176275922027997 Loss_derivative= 13.009920364744882\n",
            "t= 2.1632660016632523 Loss_derivative= 11.85869619831325\n",
            "t= 2.151407305464939 Loss_derivative= 10.809341762381033\n",
            "t= 2.140597963702558 Loss_derivative= 9.85284278996663\n",
            "t= 2.130745120912591 Loss_derivative= 8.980982670161488\n",
            "t= 2.1217641382424297 Loss_derivative= 8.186271864996886\n",
            "t= 2.1135778663774327 Loss_derivative= 7.461883572082901\n",
            "t= 2.10611598280535 Loss_derivative= 6.8015950803439775\n",
            "t= 2.099314387725006 Loss_derivative= 6.199734315078023\n",
            "t= 2.093114653409928 Loss_derivative= 5.651131113146457\n",
            "t= 2.0874635222967814 Loss_derivative= 5.151072809733796\n",
            "t= 2.0823124494870475 Loss_derivative= 4.695263755153853\n",
            "t= 2.0776171857318935 Loss_derivative= 4.279788413940257\n",
            "t= 2.073337397317953 Loss_derivative= 3.901077729231249\n",
            "t= 2.0694363195887218 Loss_derivative= 3.555878463508647\n",
            "t= 2.065880441125213 Loss_derivative= 3.24122525231928\n",
            "t= 2.062639215872894 Loss_derivative= 2.9544151309114826\n",
            "t= 2.059684800741983 Loss_derivative= 2.6929843149632524\n",
            "t= 2.0569918164270193 Loss_derivative= 2.4546870359415864\n",
            "t= 2.0545371293910777 Loss_derivative= 2.23747624928219\n",
            "t= 2.0522996531417954 Loss_derivative= 2.0394860496672025\n",
            "t= 2.0502601670921283 Loss_derivative= 1.8590156423433024\n",
            "t= 2.048401151449785 Loss_derivative= 1.694514732788204\n",
            "t= 2.046706636716997 Loss_derivative= 1.5445702092193587\n",
            "t= 2.0451620665077774 Loss_derivative= 1.407894003543081\n",
            "t= 2.0437541725042343 Loss_derivative= 1.2833120264661626\n",
            "t= 2.042470860477768 Loss_derivative= 1.1697540817193302\n",
            "t= 2.0413011063960487 Loss_derivative= 1.0662446727527444\n",
            "t= 2.0402348617232957 Loss_derivative= 0.9718946229301089\n",
            "t= 2.0392629671003655 Loss_derivative= 0.8858934372368916\n",
            "t= 2.0383770736631286 Loss_derivative= 0.8075023398867374\n",
            "t= 2.037569571323242 Loss_derivative= 0.736047928017582\n",
            "t= 2.0368335233952246 Loss_derivative= 0.6709163869605167\n",
            "t= 2.036162607008264 Loss_derivative= 0.6115482173891633\n",
            "t= 2.035551058790875 Loss_derivative= 0.5574334290539248\n",
            "t= 2.034993625361821 Loss_derivative= 0.5081071598137923\n",
            "t= 2.034485518202007 Loss_derivative= 0.4631456823323631\n",
            "t= 2.034022372519675 Loss_derivative= 0.42216276413371623\n",
            "t= 2.0336002097555412 Loss_derivative= 0.3848063497505003\n",
            "t= 2.0332154034057908 Loss_derivative= 0.3507555364626884\n",
            "t= 2.032864647869328 Loss_derivative= 0.3197178176477413\n",
            "t= 2.0325449300516802 Loss_derivative= 0.29142657006161676\n",
            "t= 2.0322535034816185 Loss_derivative= 0.2656387634656231\n",
            "t= 2.031987864718153 Loss_derivative= 0.24213287292448804\n",
            "t= 2.0317457318452288 Loss_derivative= 0.22070697584111731\n",
            "t= 2.0315250248693877 Loss_derivative= 0.2011770173813492\n",
            "t= 2.0313238478520064 Loss_derivative= 0.18337522938828332\n",
            "t= 2.031140472622618 Loss_derivative= 0.16714868920368597\n",
            "t= 2.0309733239334142 Loss_derivative= 0.15235800601700145\n",
            "t= 2.030820965927397 Loss_derivative= 0.13887612345669542\n",
            "t= 2.0306820898039404 Loss_derivative= 0.12658722813824486\n",
            "t= 2.030555502575802 Loss_derivative= 0.11538575479262536\n",
            "t= 2.0304401168210093 Loss_derivative= 0.10517547942928429\n",
            "t= 2.03033494134158 Loss_derivative= 0.09586869274338938\n",
            "t= 2.0302390726488366 Loss_derivative= 0.08738544666686132\n",
            "t= 2.03015168720217 Loss_derivative= 0.07965286759053192\n",
            "t= 2.0300720343345793 Loss_derivative= 0.07260453035824332\n",
            "t= 2.029999429804221 Loss_derivative= 0.06617988765504068\n",
            "t= 2.0299332499165663 Loss_derivative= 0.06032374988754752\n",
            "t= 2.0298729261666786 Loss_derivative= 0.05498581108902978\n",
            "t= 2.0298179403555894 Loss_derivative= 0.05012021677622047\n",
            "t= 2.029767820138813 Loss_derivative= 0.04568517004555417\n",
            "t= 2.0297221349687673 Loss_derivative= 0.04164257252538661\n",
            "t= 2.029680492396242 Loss_derivative= 0.03795769709959196\n",
            "t= 2.0296425346991422 Loss_derivative= 0.03459888959129487\n",
            "t= 2.029607935809551 Loss_derivative= 0.031537296844217244\n",
            "t= 2.029576398512707 Loss_derivative= 0.02874661886518004\n",
            "t= 2.0295476518938416 Loss_derivative= 0.026202882899624913\n",
            "t= 2.029521449010942 Loss_derivative= 0.02388423749838431\n",
            "t= 2.0294975647734437 Loss_derivative= 0.02177076480723794\n",
            "t= 2.0294757940086363 Loss_derivative= 0.01984430946658744\n",
            "t= 2.0294559496991695 Loss_derivative= 0.018088322651628075\n",
            "t= 2.029437861376518 Loss_derivative= 0.016487719912854915\n",
            "t= 2.029421373656605 Loss_derivative= 0.015028751596281031\n",
            "t= 2.029406344905009 Loss_derivative= 0.01369888472981362\n",
            "t= 2.029392646020279 Loss_derivative= 0.012486695361104805\n",
            "t= 2.029380159324918 Loss_derivative= 0.011381770422630777\n",
            "t= 2.0293687775544953 Loss_derivative= 0.01037461828026398\n",
            "t= 2.029358402936215 Loss_derivative= 0.009456587197289035\n",
            "t= 2.0293489463490175 Loss_derivative= 0.008619791013415257\n",
            "t= 2.029340326558004 Loss_derivative= 0.007857041400351483\n",
            "t= 2.029332469516604 Loss_derivative= 0.007161786111849033\n",
            "t= 2.0293253077304922 Loss_derivative= 0.006528052698006159\n",
            "t= 2.0293187796777943 Loss_derivative= 0.005950397200119473\n",
            "t= 2.029312829280594 Loss_derivative= 0.005423857385529307\n",
            "t= 2.0293074054232085 Loss_derivative= 0.00494391012046369\n",
            "t= 2.029302461513088 Loss_derivative= 0.004506432515097192\n",
            "t= 2.029297955080573 Loss_derivative= 0.0041076665065477025\n",
            "t= 2.0292938474140665 Loss_derivative= 0.0037441865760304838\n",
            "t= 2.0292901032274906 Loss_derivative= 0.003412870322828665\n",
            "t= 2.029286690357168 Loss_derivative= 0.003110871641650359\n",
            "t= 2.029283579485526 Loss_derivative= 0.002835596273924601\n",
            "t= 2.0292807438892524 Loss_derivative= 0.0025846795223106733\n",
            "t= 2.02927815920973 Loss_derivative= 0.0023559659372051067\n",
            "t= 2.029275803243793 Loss_derivative= 0.0021474908008248805\n",
            "t= 2.0292736557529922 Loss_derivative= 0.0019574632497266764\n",
            "t= 2.0292716982897425 Loss_derivative= 0.00178425089064993\n",
            "t= 2.029269914038852 Loss_derivative= 0.001626365777849581\n",
            "t= 2.029268287673074 Loss_derivative= 0.0014824516312059721\n",
            "t= 2.0292668052214426 Loss_derivative= 0.0013512721853792264\n",
            "t= 2.0292654539492574 Loss_derivative= 0.0012317005699398198\n",
            "t= 2.0292642222486874 Loss_derivative= 0.0011227096290002958\n",
            "t= 2.0292630995390586 Loss_derivative= 0.0010233630980144293\n",
            "t= 2.0292620761759608 Loss_derivative= 0.0009328075606822583\n",
            "t= 2.0292611433684002 Loss_derivative= 0.0008502651179926879\n",
            "t= 2.0292602931032824 Loss_derivative= 0.0007750267057384796\n",
            "t= 2.0292595180765765 Loss_derivative= 0.0007064460035987041\n",
            "t= 2.029258811630573 Loss_derivative= 0.0006439338829431485\n",
            "t= 2.02925816769669 Loss_derivative= 0.0005869533460184373\n",
            "t= 2.029257580743344 Loss_derivative= 0.000535014913073395\n",
            "t= 2.0292570457284307 Loss_derivative= 0.0004876724174865954\n",
            "t= 2.029256558056013 Loss_derivative= 0.00044451917315058154\n",
            "t= 2.02925611353684 Loss_derivative= 0.00040518448082915715\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, what if the engineers had a different route function that looked like this:\n",
        "$$g(x,t_1,t_2)=t_1 \\sin x +3t_2 \\cos (2x)$$\n",
        "\n",
        "1. Can you write the loss function? \\\\\n",
        "2. Can you think of a way to write its derivative?"
      ],
      "metadata": {
        "id": "qfsbPDrf4b3q"
      }
    }
  ]
}